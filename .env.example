# ==================================================
# sglang Server 環境變數配置
# 複製此檔案為 .env 並填入實際值
# ==================================================

# -------------------- API 安全 --------------------
# sglang API 金鑰 (必填，用於 API 認證)
SGLANG_API_KEY=your-secure-api-key-here

# HuggingFace Token (部分模型需要，如 Llama 系列)
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxx

# -------------------- 模型配置 --------------------
# 模型名稱 (HuggingFace 格式；供 docker-compose 與 orchestrator 共用)
# 預設：https://huggingface.co/twinkle-ai/Llama-3.2-3B-F1-Instruct
# 注意：若 HuggingFace 顯示為 gated repo，需先在模型頁面 Request access/同意條款，並在下方填入可讀取的 HF_TOKEN，否則 sglang 會 403 並 unhealthy。
SGLANG_MODEL=twinkle-ai/Llama-3.2-3B-F1-Instruct

# (optional) 權重載入格式：留空=auto；若使用 GGUF，設為 gguf
# 例：SGLANG_LOAD_FORMAT=gguf
SGLANG_LOAD_FORMAT=

# (optional) 量化方式：留空=不指定；常見：awq / gptq / bitsandbytes / gguf
# 例：SGLANG_QUANTIZATION=gguf
SGLANG_QUANTIZATION=

# (optional) 量化參數檔（部分量化需要）；用於 --quantization-param-path
SGLANG_QUANTIZATION_PARAM_PATH=

# (optional) 若你使用 HuggingFace 的 GGUF repo，可讓 compose 先下載到固定路徑 `/models_gguf/model.gguf`，
# 然後再把 SGLANG_MODEL 指到該檔案，避免下載中斷造成啟動失敗。
# 例：
#   SGLANG_GGUF_SOURCE_REPO=DevQuasar/twinkle-ai.Llama-3.2-3B-F1-Instruct-GGUF
#   SGLANG_GGUF_SOURCE_FILE=twinkle-ai.Llama-3.2-3B-F1-Instruct-Q4_K_M.gguf
#   SGLANG_MODEL=/models_gguf/model.gguf
#   SGLANG_LOAD_FORMAT=gguf
#   SGLANG_QUANTIZATION=gguf
SGLANG_GGUF_SOURCE_REPO=
SGLANG_GGUF_SOURCE_FILE=

# (recommended for GGUF) Use original HF tokenizer/chat template to improve output quality.
# Example: SGLANG_TOKENIZER_PATH=twinkle-ai/Llama-3.2-3B-F1-Instruct
SGLANG_TOKENIZER_PATH=

# LLM 輸出上限（避免模型長時間輸出造成 web UI 卡死；<=0 或留空代表不限制）
SGLANG_MAX_TOKENS=512

# System prompt (optional; reduce repetition / self-identity)
SGLANG_SYSTEM_PROMPT=請用繁體中文回答；避免重複；不要自稱 Qwen；自我介紹請 80 字內。

# Sampling (recommended defaults)
SGLANG_TEMPERATURE=0.6
SGLANG_TOP_P=0.9
SGLANG_TOP_K=20
SGLANG_REPETITION_PENALTY=1.15
SGLANG_PRESENCE_PENALTY=
SGLANG_FREQUENCY_PENALTY=

# (legacy) 舊版腳本可能使用 MODEL_NAME；此專案已改以 SGLANG_MODEL 為準
# MODEL_NAME=twinkle-ai/Llama-3.2-3B-F1-Instruct

# 最大上下文長度 (RTX 4060 Ti 8GB 建議 2048-4096；若遇到 Not enough memory，先降到 2048)
MAX_MODEL_LEN=2048

# SGLang GPU 記憶體使用比例（越大越能分到 KV cache；若遇到 Not enough memory，先升到 0.95）
SGLANG_MEM_FRACTION_STATIC=0.95

# 模型快取路徑 (Windows 格式需調整)
MODEL_CACHE_PATH=~/.cache/huggingface

# -------------------- SGLang / Orchestrator 連線參數（本機 dev 用；compose 內通常不需要改） --------------------
# Orchestrator 會呼叫這個 base URL（compose 內已固定指到容器：http://sglang:30000）
SGLANG_BASE_URL=http://localhost:8082

# -------------------- Grafana (監控用) --------------------
GRAFANA_USER=admin
GRAFANA_PASSWORD=your-grafana-password

# -------------------- Orchestrator (可選) --------------------
# 本機啟動 orchestrator 用（compose 內已固定 ORCH_HOST/ORCH_PORT）
ORCH_HOST=0.0.0.0
ORCH_PORT=9100

# 若設定，前端連 Orchestrator 時需帶 ?api_key= 或 Authorization: Bearer
ORCH_API_KEY=

# -------------------- ws_gateway_tts (預設 piper；第一次啟動會自動下載到 volume) --------------------
# 本機啟動 ws_gateway_tts 用（compose 內已固定 WS_TTS_HOST/WS_TTS_PORT 與 WS_TTS_URL）
WS_TTS_HOST=0.0.0.0
WS_TTS_PORT=9000
# Orchestrator 會連到此 WS 端點（本機 dev 用；compose 內已固定 ws://ws_gateway_tts:9000/tts）
WS_TTS_URL=ws://localhost:9000/tts

WS_TTS_ENGINE=piper
# Optional: protect gateway endpoint (Bearer)
WS_TTS_API_KEY=

# delta -> TTS buffering policy (Orchestrator)
TTS_FLUSH_MIN_CHARS=12
TTS_FLUSH_ON_PUNCT=true
ALLOW_CLIENT_TTS_URL=false

# Piper (if WS_TTS_ENGINE=piper)
PIPER_BIN=/opt/piper/piper
PIPER_MODEL=/opt/piper/models/zh_CN-huayan-medium.onnx
PIPER_SPEAKER_ID=
PIPER_EXTRA_ARGS=
PIPER_OUTPUT_MODE=file

# Riva (if WS_TTS_ENGINE=riva)
RIVA_SERVER=localhost:50051

# -------------------- Legacy / Deprecated --------------------
# 舊文件曾使用 GATEWAY_API_KEY；目前程式碼不讀取此參數（請改用 ORCH_API_KEY 或 WS_TTS_API_KEY）
GATEWAY_API_KEY=

# -------------------- Debug: use local orchestrator with web(nginx) --------------------
# If you want http://localhost:8080/ to proxy /chat to your local orchestrator (VSCode F5),
# set this and restart the web container: docker compose up -d --build --force-recreate web
# (use --no-deps so compose won't start the orchestrator container again)
# ORCHESTRATOR_UPSTREAM=host.docker.internal:9100
