name: sglang

services:
  # GGUF 模型下載服務
  model_fetch:
    image: lmsysorg/sglang:latest
    container_name: sglang-model-fetch
    env_file: .env
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}
      - SGLANG_GGUF_SOURCE_REPO=${SGLANG_GGUF_SOURCE_REPO:-}
      - SGLANG_GGUF_SOURCE_FILE=${SGLANG_GGUF_SOURCE_FILE:-}
    volumes:
      - ./models:/root/.cache/huggingface
      - ./models_gguf:/models_gguf
    command: |
      bash -lc '
        set -euo pipefail
        mkdir -p /models_gguf
        
        # 若已有模型，直接就緒
        if [ -f /models_gguf/model.gguf ]; then
          echo "[model_fetch] model.gguf already exists, skipping download"
          touch /models_gguf/.ready
          sleep infinity
        fi
        
        # 若未設定來源，跳過
        if [ -z "${SGLANG_GGUF_SOURCE_REPO:-}" ] || [ -z "${SGLANG_GGUF_SOURCE_FILE:-}" ]; then
          echo "[model_fetch] skip (SGLANG_GGUF_SOURCE_REPO or SGLANG_GGUF_SOURCE_FILE not set)"
          touch /models_gguf/.ready
          sleep infinity
        fi
        
        # 下載模型
        rm -f /models_gguf/.ready
        echo "[model_fetch] downloading ${SGLANG_GGUF_SOURCE_REPO} / ${SGLANG_GGUF_SOURCE_FILE}"
        huggingface-cli download "${SGLANG_GGUF_SOURCE_REPO}" "${SGLANG_GGUF_SOURCE_FILE}" \
          --local-dir /tmp/gguf --local-dir-use-symlinks False
        
        cp -f "/tmp/gguf/${SGLANG_GGUF_SOURCE_FILE}" /models_gguf/model.gguf
        touch /models_gguf/.ready
        echo "[model_fetch] ready: /models_gguf/model.gguf"
        sleep infinity
      '
    healthcheck:
      test: [ "CMD", "bash", "-lc", "test -f /models_gguf/.ready" ]
      interval: 10s
      timeout: 5s
      retries: 100
      start_period: 5s

  # SGLang LLM 推論服務
  sglang:
    image: lmsysorg/sglang:latest
    container_name: sglang-server
    runtime: nvidia
    env_file: .env
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}
      - SGLANG_API_KEY=${SGLANG_API_KEY}
    ports:
      - "8082:30000"
    volumes:
      - ./models:/root/.cache/huggingface
      - ./models_gguf:/models_gguf
      - ./logs:/app/logs
    depends_on:
      model_fetch:
        condition: service_healthy
    command: |
      bash -lc '
        set -euo pipefail
        python3 -m sglang.launch_server \
          --model-path "${SGLANG_MODEL:-/models_gguf/model.gguf}" \
          --tokenizer-path "${SGLANG_TOKENIZER_PATH:-}" \
          --port 30000 --host 0.0.0.0 \
          --mem-fraction-static "${SGLANG_MEM_FRACTION_STATIC:-0.95}" \
          --context-length "${MAX_MODEL_LEN:-2048}" \
          --api-key "${SGLANG_API_KEY}" \
          --load-format "${SGLANG_LOAD_FORMAT:-auto}" \
          --quantization "${SGLANG_QUANTIZATION:-}" \
          --kv-cache-dtype "${SGLANG_KV_CACHE_DTYPE:-auto}"
      '
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:30000/health').read()" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s

  # WebSocket TTS Gateway (Piper 語音合成)
  ws_gateway_tts:
    build:
      context: .
      dockerfile: docker/ws_gateway_tts.Dockerfile
    container_name: ws-gateway-tts
    environment:
      - WS_TTS_HOST=0.0.0.0
      - WS_TTS_PORT=9000
      - WS_TTS_ENGINE=${WS_TTS_ENGINE:-piper}
      - PIPER_ROOT=/opt/piper
      - PIPER_BIN=/opt/piper/piper
      - PIPER_MODEL=/opt/piper/models/zh_CN-huayan-medium.onnx
      - PIPER_RELEASE_TAG=2023.11.14-2
      - PIPER_TARBALL_URL=https://github.com/rhasspy/piper/releases/download/2023.11.14-2/piper_linux_x86_64.tar.gz
      - PIPER_TARBALL_SHA256=A50CB45F355B7AF1F6D758C1B360717877BA0A398CC8CBE6D2A7A3A26E225992
      - PIPER_DEFAULT_MODEL_NAME=zh_CN-huayan-medium
      - PIPER_MODEL_ONNX_URL=https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx
      - PIPER_MODEL_ONNX_SHA256=9929917BF8CABB26FD528EA44D3A6699C11E87317A14765312420BE230BE0F3D
      - PIPER_MODEL_JSON_URL=https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx.json
      - PIPER_MODEL_JSON_SHA256=D521DC45504A8CCC99E325822B35946DD701840BFB07E3DBB31A40929ED6A82B
    volumes:
      - piper-data:/opt/piper
    ports:
      - "9000:9000"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9000/healthz').read()" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # Orchestrator (Web ↔ SGLang ↔ TTS 協調器)
  orchestrator:
    build:
      context: .
      dockerfile: docker/orchestrator.Dockerfile
    container_name: orchestrator
    depends_on:
      sglang:
        condition: service_healthy
    environment:
      - ORCH_HOST=0.0.0.0
      - ORCH_PORT=9100
      - ORCH_API_KEY=${ORCH_API_KEY:-}
      - SGLANG_BASE_URL=http://sglang:30000
      - SGLANG_API_KEY=${SGLANG_API_KEY}
      - SGLANG_MODEL=${SGLANG_MODEL:-/models_gguf/model.gguf}
      - SGLANG_MAX_TOKENS=${SGLANG_MAX_TOKENS:-512}
      - SGLANG_SYSTEM_PROMPT=${SGLANG_SYSTEM_PROMPT:-請用繁體中文回答}
      - SGLANG_TEMPERATURE=${SGLANG_TEMPERATURE:-0.6}
      - SGLANG_TOP_P=${SGLANG_TOP_P:-0.9}
      - SGLANG_TOP_K=${SGLANG_TOP_K:-20}
      - SGLANG_REPETITION_PENALTY=${SGLANG_REPETITION_PENALTY:-1.15}
      - WS_TTS_URL=ws://ws_gateway_tts:9000/tts
      - WS_TTS_API_KEY=${WS_TTS_API_KEY:-}
      - TTS_FLUSH_MIN_CHARS=12
      - TTS_FLUSH_ON_PUNCT=true
    ports:
      - "9100:9100"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9100/healthz').read()" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # SAGA Server (Workflow + Observability)
  saga_server:
    build:
      context: .
      dockerfile: docker/saga_server.Dockerfile
    container_name: saga-server
    env_file: .env
    environment:
      - SGLANG_URL=http://sglang-server:30000/v1/chat/completions
      - SGLANG_BASE_URL=http://sglang-server:30000
      - SGLANG_API_KEY=${SGLANG_API_KEY}
      - SAGA_USE_LLM_MODULES=true
      - SAGA_USE_GROQ=${SAGA_USE_GROQ:-false}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-openai/gpt-oss-120b}
    ports:
      - "9200:9200"
    restart: unless-stopped
    depends_on:
      sglang:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9200/healthz').read()" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # Web (Nginx 反向代理 + 前端靜態檔)
  web:
    build:
      context: .
      dockerfile: docker/web.Dockerfile
    container_name: web
    depends_on:
      orchestrator:
        condition: service_healthy
      ws_gateway_tts:
        condition: service_healthy
      sglang:
        condition: service_healthy
      saga_server:
        condition: service_healthy
    ports:
      - "8080:80"
    environment:
      - ORCHESTRATOR_UPSTREAM=${ORCHESTRATOR_UPSTREAM:-orchestrator:9100}
      - SAGA_UPSTREAM=${SAGA_UPSTREAM:-saga_server:9200}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "-qO-", "http://127.0.0.1/healthz" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 5s

volumes:
  piper-data:
