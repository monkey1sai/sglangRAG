name: sglang
services:
  # ===========================================
  # Optional: fetch/prepare GGUF into a fixed path
  # ===========================================
  model_fetch:
    image: lmsysorg/sglang:latest
    container_name: sglang-model-fetch
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}
      - SGLANG_GGUF_SOURCE_REPO=${SGLANG_GGUF_SOURCE_REPO:-}
      - SGLANG_GGUF_SOURCE_FILE=${SGLANG_GGUF_SOURCE_FILE:-}
    volumes:
      - ./sglang-server/models:/root/.cache/huggingface
      - ./sglang-server/models_gguf:/models_gguf
    command: >
      bash -lc '
        set -euo pipefail;
        mkdir -p /models_gguf;
        if [ -z "${SGLANG_GGUF_SOURCE_REPO}" ] || [ -z "${SGLANG_GGUF_SOURCE_FILE}" ]; then
          echo "[model_fetch] skip (SGLANG_GGUF_SOURCE_REPO/SGLANG_GGUF_SOURCE_FILE not set)";
          touch /models_gguf/.ready;
          sleep infinity;
        fi;
        echo "[model_fetch] downloading ${SGLANG_GGUF_SOURCE_REPO} ${SGLANG_GGUF_SOURCE_FILE}";
        rm -rf /tmp/gguf && mkdir -p /tmp/gguf;
        huggingface-cli download "${SGLANG_GGUF_SOURCE_REPO}" "${SGLANG_GGUF_SOURCE_FILE}" --local-dir /tmp/gguf --local-dir-use-symlinks False;
        cp -f "/tmp/gguf/${SGLANG_GGUF_SOURCE_FILE}" /models_gguf/model.gguf;
        touch /models_gguf/.ready;
        echo "[model_fetch] ready: /models_gguf/model.gguf";
        sleep infinity;
      '
    healthcheck:
      test: ["CMD", "bash", "-lc", "test -f /models_gguf/.ready"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 5s

  # ===========================================
  # SGLang Server (Optimized for Tool Use & Speed)
  # ===========================================
  sglang:
    image: lmsysorg/sglang:latest
    container_name: sglang-server
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}
      - SGLANG_API_KEY=${SGLANG_API_KEY}
    ports:
      - "8082:30000" # Mapped to host 8082 to avoid conflict with sglang (8081)
    volumes:
      # Reuse the same model cache as sglang
      - ./sglang-server/models:/root/.cache/huggingface
      - ./sglang-server/models_gguf:/models_gguf
      - ./sglang-server/logs:/app/logs
    depends_on:
      model_fetch:
        condition: service_healthy
    command: >
      bash -lc '
        set -euo pipefail;
        ARGS=(python3 -m sglang.launch_server);
        ARGS+=(--model-path "${SGLANG_MODEL:-twinkle-ai/Llama-3.2-3B-F1-Instruct}");
        if [ -n "${SGLANG_TOKENIZER_PATH:-}" ]; then ARGS+=(--tokenizer-path "${SGLANG_TOKENIZER_PATH}"); fi;
        ARGS+=(--port 30000 --host 0.0.0.0);
        ARGS+=(--mem-fraction-static "${SGLANG_MEM_FRACTION_STATIC:-0.95}");
        ARGS+=(--context-length "${MAX_MODEL_LEN:-2048}");
        ARGS+=(--api-key "${SGLANG_API_KEY}");
        if [ -n "${SGLANG_LOAD_FORMAT:-}" ]; then ARGS+=(--load-format "${SGLANG_LOAD_FORMAT}"); fi;
        if [ -n "${SGLANG_QUANTIZATION:-}" ]; then ARGS+=(--quantization "${SGLANG_QUANTIZATION}"); fi;
        if [ -n "${SGLANG_QUANTIZATION_PARAM_PATH:-}" ]; then ARGS+=(--quantization-param-path "${SGLANG_QUANTIZATION_PARAM_PATH}"); fi;
        if [ -n "${SGLANG_KV_CACHE_DTYPE:-}" ]; then ARGS+=(--kv-cache-dtype "${SGLANG_KV_CACHE_DTYPE}"); fi;
        if [ -n "${SGLANG_MAX_TOTAL_TOKENS:-}" ]; then ARGS+=(--max-total-tokens "${SGLANG_MAX_TOTAL_TOKENS}"); fi;
        case "${SGLANG_ALLOW_AUTO_TRUNCATE:-}" in 1|true|yes|y|on) ARGS+=(--allow-auto-truncate) ;; esac;
        exec "$${ARGS[@]}";
      '
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "python3",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:30000/health').read(); print('ok')",
        ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s

  # ===========================================
  # WebSocket TTS Gateway (Piper by default; first boot auto-download)
  # ===========================================
  ws_gateway_tts:
    build:
      context: .
      dockerfile: docker/ws_gateway_tts.Dockerfile
    container_name: ws-gateway-tts
    environment:
      - WS_TTS_HOST=0.0.0.0
      - WS_TTS_PORT=9000
      - WS_TTS_ENGINE=${WS_TTS_ENGINE:-piper}
      # Piper (default; first boot auto-download into named volume)
      - PIPER_ROOT=/opt/piper
      - PIPER_BIN=${PIPER_BIN:-/opt/piper/piper}
      - PIPER_MODEL=${PIPER_MODEL:-/opt/piper/models/zh_CN-huayan-medium.onnx}
      - PIPER_RELEASE_TAG=2023.11.14-2
      - PIPER_TARBALL_URL=https://github.com/rhasspy/piper/releases/download/2023.11.14-2/piper_linux_x86_64.tar.gz
      - PIPER_TARBALL_SHA256=A50CB45F355B7AF1F6D758C1B360717877BA0A398CC8CBE6D2A7A3A26E225992
      - PIPER_DEFAULT_MODEL_NAME=zh_CN-huayan-medium
      - PIPER_MODEL_ONNX_URL=https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx
      - PIPER_MODEL_ONNX_SHA256=9929917BF8CABB26FD528EA44D3A6699C11E87317A14765312420BE230BE0F3D
      - PIPER_MODEL_JSON_URL=https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/zh/zh_CN/huayan/medium/zh_CN-huayan-medium.onnx.json
      - PIPER_MODEL_JSON_SHA256=D521DC45504A8CCC99E325822B35946DD701840BFB07E3DBB31A40929ED6A82B
      - PIPER_SPEAKER_ID=${PIPER_SPEAKER_ID:-}
      - PIPER_EXTRA_ARGS=${PIPER_EXTRA_ARGS:-}
      - PIPER_OUTPUT_MODE=${PIPER_OUTPUT_MODE:-file}
      # Riva (optional)
      - RIVA_SERVER=${RIVA_SERVER:-localhost:50051}
    volumes:
      - piper-data:/opt/piper
    ports:
      - "9000:9000"
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:9000/healthz').read(); print('ok')",
        ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # ===========================================
  # Orchestrator (Web client ↔ SGLang ↔ ws_gateway_tts)
  # ===========================================
  orchestrator:
    build:
      context: .
      dockerfile: docker/orchestrator.Dockerfile
    container_name: orchestrator
    depends_on:
      sglang:
        condition: service_healthy
      ws_gateway_tts:
        condition: service_healthy
    environment:
      - ORCH_HOST=0.0.0.0
      - ORCH_PORT=9100
      - ORCH_API_KEY=${ORCH_API_KEY:-}
      - SGLANG_BASE_URL=http://sglang:30000
      - SGLANG_API_KEY=${SGLANG_API_KEY}
      - SGLANG_MODEL=${SGLANG_MODEL:-twinkle-ai/Llama-3.2-3B-F1-Instruct}
      - SGLANG_MAX_TOKENS=${SGLANG_MAX_TOKENS:-512}
      - SGLANG_SYSTEM_PROMPT=${SGLANG_SYSTEM_PROMPT:-Answer in Traditional Chinese. Do not claim to be Qwen. Avoid repeating phrases. Keep self-intro under 80 chars.}
      - SGLANG_TEMPERATURE=${SGLANG_TEMPERATURE:-0.5}
      - SGLANG_TOP_P=${SGLANG_TOP_P:-0.85}
      - SGLANG_TOP_K=${SGLANG_TOP_K:-20}
      - SGLANG_REPETITION_PENALTY=${SGLANG_REPETITION_PENALTY:-1.2}
      - SGLANG_PRESENCE_PENALTY=${SGLANG_PRESENCE_PENALTY:-0.2}
      - SGLANG_FREQUENCY_PENALTY=${SGLANG_FREQUENCY_PENALTY:-0.2}
      - WS_TTS_URL=ws://ws_gateway_tts:9000/tts
      - WS_TTS_API_KEY=${WS_TTS_API_KEY:-}
      - TTS_FLUSH_MIN_CHARS=${TTS_FLUSH_MIN_CHARS:-12}
      - TTS_FLUSH_ON_PUNCT=${TTS_FLUSH_ON_PUNCT:-true}
      - ALLOW_CLIENT_TTS_URL=${ALLOW_CLIENT_TTS_URL:-false}
    ports:
      - "9100:9100"
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:9100/healthz').read(); print('ok')",
        ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # ===========================================
  # Web client (static) + reverse proxy (same origin)
  # ===========================================
  web:
    build:
      context: .
      dockerfile: docker/web.Dockerfile
    container_name: web
    depends_on:
      orchestrator:
        condition: service_healthy
      ws_gateway_tts:
        condition: service_healthy
      sglang:
        condition: service_healthy
    ports:
      - "8080:80"
    environment:
      - ORCHESTRATOR_UPSTREAM=${ORCHESTRATOR_UPSTREAM:-orchestrator:9100}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://127.0.0.1/healthz"]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 5s

volumes:
  piper-data:
